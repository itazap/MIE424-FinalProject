{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Balanced_Copy_of_MIE424_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHHafe9oBNJn"
      },
      "source": [
        "# MIE424 Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktA8StDVa56K"
      },
      "source": [
        "## Section 4. Load and Explore the Adult Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0c0JuKqBYkW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from random import shuffle"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAB6xPc9A9WD",
        "outputId": "25a33678-b234-4266-8d9d-ec4d9c26ea43"
      },
      "source": [
        "!git clone https://github.com/mlohaus/SearchFair.git\n",
        "%cd SearchFair"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SearchFair'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/86)\u001b[K\rremote: Counting objects:   2% (2/86)\u001b[K\rremote: Counting objects:   3% (3/86)\u001b[K\rremote: Counting objects:   4% (4/86)\u001b[K\rremote: Counting objects:   5% (5/86)\u001b[K\rremote: Counting objects:   6% (6/86)\u001b[K\rremote: Counting objects:   8% (7/86)\u001b[K\rremote: Counting objects:   9% (8/86)\u001b[K\rremote: Counting objects:  10% (9/86)\u001b[K\rremote: Counting objects:  11% (10/86)\u001b[K\rremote: Counting objects:  12% (11/86)\u001b[K\rremote: Counting objects:  13% (12/86)\u001b[K\rremote: Counting objects:  15% (13/86)\u001b[K\rremote: Counting objects:  16% (14/86)\u001b[K\rremote: Counting objects:  17% (15/86)\u001b[K\rremote: Counting objects:  18% (16/86)\u001b[K\rremote: Counting objects:  19% (17/86)\u001b[K\rremote: Counting objects:  20% (18/86)\u001b[K\rremote: Counting objects:  22% (19/86)\u001b[K\rremote: Counting objects:  23% (20/86)\u001b[K\rremote: Counting objects:  24% (21/86)\u001b[K\rremote: Counting objects:  25% (22/86)\u001b[K\rremote: Counting objects:  26% (23/86)\u001b[K\rremote: Counting objects:  27% (24/86)\u001b[K\rremote: Counting objects:  29% (25/86)\u001b[K\rremote: Counting objects:  30% (26/86)\u001b[K\rremote: Counting objects:  31% (27/86)\u001b[K\rremote: Counting objects:  32% (28/86)\u001b[K\rremote: Counting objects:  33% (29/86)\u001b[K\rremote: Counting objects:  34% (30/86)\u001b[K\rremote: Counting objects:  36% (31/86)\u001b[K\rremote: Counting objects:  37% (32/86)\u001b[K\rremote: Counting objects:  38% (33/86)\u001b[K\rremote: Counting objects:  39% (34/86)\u001b[K\rremote: Counting objects:  40% (35/86)\u001b[K\rremote: Counting objects:  41% (36/86)\u001b[K\rremote: Counting objects:  43% (37/86)\u001b[K\rremote: Counting objects:  44% (38/86)\u001b[K\rremote: Counting objects:  45% (39/86)\u001b[K\rremote: Counting objects:  46% (40/86)\u001b[K\rremote: Counting objects:  47% (41/86)\u001b[K\rremote: Counting objects:  48% (42/86)\u001b[K\rremote: Counting objects:  50% (43/86)\u001b[K\rremote: Counting objects:  51% (44/86)\u001b[K\rremote: Counting objects:  52% (45/86)\u001b[K\rremote: Counting objects:  53% (46/86)\u001b[K\rremote: Counting objects:  54% (47/86)\u001b[K\rremote: Counting objects:  55% (48/86)\u001b[K\rremote: Counting objects:  56% (49/86)\u001b[K\rremote: Counting objects:  58% (50/86)\u001b[K\rremote: Counting objects:  59% (51/86)\u001b[K\rremote: Counting objects:  60% (52/86)\u001b[K\rremote: Counting objects:  61% (53/86)\u001b[K\rremote: Counting objects:  62% (54/86)\u001b[K\rremote: Counting objects:  63% (55/86)\u001b[K\rremote: Counting objects:  65% (56/86)\u001b[K\rremote: Counting objects:  66% (57/86)\u001b[K\rremote: Counting objects:  67% (58/86)\u001b[K\rremote: Counting objects:  68% (59/86)\u001b[K\rremote: Counting objects:  69% (60/86)\u001b[K\rremote: Counting objects:  70% (61/86)\u001b[K\rremote: Counting objects:  72% (62/86)\u001b[K\rremote: Counting objects:  73% (63/86)\u001b[K\rremote: Counting objects:  74% (64/86)\u001b[K\rremote: Counting objects:  75% (65/86)\u001b[K\rremote: Counting objects:  76% (66/86)\u001b[K\rremote: Counting objects:  77% (67/86)\u001b[K\rremote: Counting objects:  79% (68/86)\u001b[K\rremote: Counting objects:  80% (69/86)\u001b[K\rremote: Counting objects:  81% (70/86)\u001b[K\rremote: Counting objects:  82% (71/86)\u001b[K\rremote: Counting objects:  83% (72/86)\u001b[K\rremote: Counting objects:  84% (73/86)\u001b[K\rremote: Counting objects:  86% (74/86)\u001b[K\rremote: Counting objects:  87% (75/86)\u001b[K\rremote: Counting objects:  88% (76/86)\u001b[K\rremote: Counting objects:  89% (77/86)\u001b[K\rremote: Counting objects:  90% (78/86)\u001b[K\rremote: Counting objects:  91% (79/86)\u001b[K\rremote: Counting objects:  93% (80/86)\u001b[K\rremote: Counting objects:  94% (81/86)\u001b[K\rremote: Counting objects:  95% (82/86)\u001b[K\rremote: Counting objects:  96% (83/86)\u001b[K\rremote: Counting objects:  97% (84/86)\u001b[K\rremote: Counting objects:  98% (85/86)\u001b[K\rremote: Counting objects: 100% (86/86)\u001b[K\rremote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 86 (delta 32), reused 74 (delta 24), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n",
            "/content/SearchFair/SearchFair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "IiXEhiXIBhBO",
        "outputId": "c21faba3-14d9-47e5-8a62-5905ef58a813"
      },
      "source": [
        "# Load data into pandas DataFrame\n",
        "dataset = pd.read_csv('data/adult/adult.csv')\n",
        "data50minus = dataset[dataset[\"income\"].str.contains(\"<=50K\")].iloc[:7841]\n",
        "data50plus = dataset[dataset[\"income\"].str.contains(\">50K\")].iloc[:7841]\n",
        "databalanced = pd.concat([data50minus, data50plus])\n",
        "dataset = databalanced.sample(frac=1).reset_index(drop=True)\n",
        "# Drop fnlwgt, education, education-num, capital-gain, capital-loss as Lohaus et al do\n",
        "dataset = dataset.drop(columns=['fnlwgt', 'education', 'capital-gain', 'capital-loss'])\n",
        "dataset"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36</td>\n",
              "      <td>Private</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Transport-moving</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>?</td>\n",
              "      <td>10</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>?</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>36</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>69</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>14</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>11</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>Private</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>Mexico</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15677</th>\n",
              "      <td>50</td>\n",
              "      <td>Private</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>50</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15678</th>\n",
              "      <td>47</td>\n",
              "      <td>?</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15679</th>\n",
              "      <td>37</td>\n",
              "      <td>Private</td>\n",
              "      <td>9</td>\n",
              "      <td>Separated</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15680</th>\n",
              "      <td>37</td>\n",
              "      <td>Private</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Craft-repair</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15681</th>\n",
              "      <td>30</td>\n",
              "      <td>Private</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>25</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15682 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  workclass  education-num  ... hours-per-week native-country income\n",
              "0       36    Private              9  ...             50  United-States   >50K\n",
              "1       22          ?             10  ...             36  United-States  <=50K\n",
              "2       69  State-gov             14  ...             11  United-States  <=50K\n",
              "3       45    Private             13  ...             40  United-States   >50K\n",
              "4       58    Private             13  ...             40         Mexico  <=50K\n",
              "...    ...        ...            ...  ...            ...            ...    ...\n",
              "15677   50    Private              9  ...             50  United-States   >50K\n",
              "15678   47          ?              9  ...             40  United-States  <=50K\n",
              "15679   37    Private              9  ...             40  United-States  <=50K\n",
              "15680   37    Private             10  ...             40  United-States   >50K\n",
              "15681   30    Private             10  ...             25  United-States   >50K\n",
              "\n",
              "[15682 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOGPaMY_EQd7"
      },
      "source": [
        "def build_adult_data(dataset=dataset,sens_attribute = 'sex',load_data_size=None):\n",
        "  \"\"\"Build the Adult dataset.\n",
        "  Source: UCI Machine Learning Repository.\n",
        "  All Binary Mappings are defined in Lohaus' Too Relaxed To Be Fair research.\n",
        "  We have kept the exact same mapping to make for clean comparisons. \n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  load_data_size: int\n",
        "      The number of points to be loaded. If None, returns all data points unshuffled.\n",
        "      If other than None, returns load_data_size shuffled.\n",
        "\n",
        "  Returns\n",
        "  ---------\n",
        "  X: numpy array\n",
        "      The feature input matrix after a binary mapping of attributes.\n",
        "      Shape=(number_data_points, number_features)\n",
        "  y: numpy array\n",
        "      The classification labels (matrix) after a binary mapping of attributes.\n",
        "      Shape=(number_data_points,).\n",
        "  s: numpy array\n",
        "      The sensitive feature vector after a binary mapping of attributes. \n",
        "      Shape=(number_data_points,).\n",
        "  \"\"\"\n",
        "  \n",
        "  def binary_mapping(tuple):\n",
        "    # 'age'- Binary Cut-off: 37\n",
        "    tuple['age'] = 1 if tuple['age'] > 37 else 0\n",
        "    # 'workclass'- Binary Translation to Private/NonPrivate\n",
        "    tuple['workclass'] = 'NonPrivate' if tuple['workclass'] != 'Private' else 'Private'\n",
        "    # 'education-num'- Binary Cut-off: 9\n",
        "    tuple['education-num'] = 1 if tuple['education-num'] > 9 else 0\n",
        "    # 'maritial-status'- Binary Translation to Married-civ-spouse/nonMarriedcivspouse\n",
        "    tuple['marital-status'] = \"Marriedcivspouse\" if tuple['marital-status'] == \"Married-civ-spouse\" else \"nonMarriedcivspouse\"\n",
        "    # 'occupation'- Binary Translation to Craft-repair/NonCraftrepair\n",
        "    tuple['occupation'] = \"Craftrepair\" if tuple['occupation'] == \"Craft-repair\" else \"NonCraftrepair\"\n",
        "    # 'relationship'- Binary Translation to InFamily/Not-in-family\n",
        "    tuple['relationship'] = \"NotInFamily\" if tuple['relationship'] == \"Not-in-family\" else \"InFamily\"\n",
        "    # 'race'- Binary Translation to White/NonWhite\n",
        "    tuple['race'] = 'NonWhite' if tuple['race'] != \"White\" else \"White\"\n",
        "    # 'sex'- Binary Translation to Male/Female\n",
        "    tuple['sex'] = 'Female' if tuple['sex'] != \"Male\" else 'Male'\n",
        "    # 'hours-per-week'- Binary Cut-off: 40\n",
        "    tuple['hours-per-week'] = 1 if tuple['hours-per-week'] > 40 else 0\n",
        "    # 'native-country'- Binary Translation to United-States/NonUS\n",
        "    tuple['native-country'] = \"US\" if tuple['native-country'] == \"United-States\" else \"NonUS\"\n",
        "    \n",
        "    return tuple\n",
        "\n",
        "  df = dataset\n",
        "  df = df.apply(binary_mapping, axis=1)\n",
        "\n",
        "  # Convert Binary Mapping of Sensitive Attribute to {1,-1}\n",
        "  if sens_attribute == 'sex':\n",
        "    sensitive_attr_map = {'Male': 1, 'Female': -1}\n",
        "    x_vars = ['age','workclass','education-num','marital-status','occupation','relationship','race','hours-per-week','native-country']\n",
        "  elif sens_attribute == 'race':\n",
        "    sensitive_attr_map = {'White': 1, 'NonWhite': -1}\n",
        "    x_vars = ['age','workclass','education-num','marital-status','occupation','relationship','sex','hours-per-week','native-country']\n",
        "  \n",
        "  s = df[sens_attribute].map(sensitive_attr_map).astype(int)\n",
        "\n",
        "  # Convert Binary Mapping of Label Attribute to {1,-1}\n",
        "  label_map = {'>50K': 1, '<=50K': -1}\n",
        "  y = df['income'].map(label_map).astype(int)\n",
        "\n",
        "  # Build Input Matrix (Feature Set) as a proper DataFrame\n",
        "  x = pd.DataFrame(data=None)\n",
        "  for x_var in x_vars:\n",
        "    x = pd.concat([x, pd.get_dummies(df[x_var],prefix=x_var, drop_first=False)], axis=1)\n",
        "\n",
        "  # Return as numpy objects: Matrix/Vectors\n",
        "  X = x.to_numpy()\n",
        "  s = s.to_numpy()\n",
        "  y = y.to_numpy()\n",
        "\n",
        "  if load_data_size is not None:\n",
        "      # Shuffle the data only if data_size is specified (Random detail from Paper code)\n",
        "      perm = list(range(0, len(y)))\n",
        "      shuffle(perm)\n",
        "      X = X[perm][:load_data_size]\n",
        "      y = y[perm][:load_data_size]\n",
        "      s = s[perm][:load_data_size]\n",
        "\n",
        "  # X = X[:, (X != 0).any(axis=0)]\n",
        "\n",
        "  return X, y, s\n",
        "\n",
        "def normalize(x):\n",
        "\t# scale to [-1, 1]\n",
        "\tx_ = (x - x.min()) / (x.max() - x.min()) * 2 - 1\n",
        "\treturn x_"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUAX-dZ6ESo_"
      },
      "source": [
        "# Test\n",
        "# build_adult_data(sens_attribute='race',load_data_size=None)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPZesAQRPqBX"
      },
      "source": [
        "## Section 5.1 Implement Baseline Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INV5lNE9PuPP"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import KFold\n",
        "import sklearn.metrics.pairwise as kernels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, GroupShuffleSplit\n",
        "\n",
        "import cvxpy as cp"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAPO8nTeqKhi"
      },
      "source": [
        "class BaselineModel(BaseEstimator):\n",
        "    def __init__(self, l2_beta=0.001, kernel='linear', gamma=0.1, loss_name='hinge', lambda_max=1, max_iter=3000, solver='SCS', verbose=False,reason_points=0.5):\n",
        "\n",
        "        self.l2_beta = l2_beta # Regularization parameter beta for the l2 regularization\n",
        "        self.kernel = kernel # The SVM kernel to be used.. Options:['linear','rbf','poly']\n",
        "        self.gamma = gamma # If kernel='rbf', gamma to be kernel width, If kernel='poly', gamma to be degree.\n",
        "        self.loss_name = loss_name # Loss function to be used. Options:['hinge','logistic','squared','exponential']\n",
        "        self.lambda_max = lambda_max # The max lambda value for the start of the binary search.\n",
        "        self.max_iter = max_iter # The number of iterations.\n",
        "        self.solver = solver # The solver to be used by cvxpy. Options:['SCS','ECOS'].\n",
        "        self.verbose = verbose # If true, Overrides the default of hiding solver output.\n",
        "        self.reason_points = reason_points # The ratio of points used as reasonable points for the similarity-based approach of SearchFair.\n",
        "\n",
        "    def fit(self, x_train, y_train,s_train):\n",
        "\n",
        "        # Set class variables\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.s_train = s_train\n",
        "        \n",
        "        # Must call preprocess() after the datapoint size is given\n",
        "        self._preprocess()\n",
        "        # Construct the CVXPY problem\n",
        "        self._construct_problem()\n",
        "        # Optimize with given settings \n",
        "        self._optimize()\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def _preprocess(self):\n",
        "       \n",
        "        # Initialize the coef_ array as a class variable using cvxpy's built-in loss functions\n",
        "        self.coef_ = None\n",
        "\n",
        "        # Generate loss function based on the given loss_name\n",
        "        if self.loss_name == 'logistic':\n",
        "          self.loss_func = lambda z: cp.logistic(-z)\n",
        "        elif self.loss_name == 'hinge':\n",
        "          self.loss_func = lambda z: cp.pos(1.0 - z)\n",
        "        elif self.loss_name == 'squared':\n",
        "          self.loss_func = lambda z: cp.square(-z)\n",
        "        elif self.loss_name == 'exponential':\n",
        "          self.loss_func = lambda z: cp.exp(-z)\n",
        "        else:\n",
        "          self.loss_func = lambda z: cp.pos(1.0 - z) # Use Hinge-Loss unless specified otherwise\n",
        "\n",
        "        if self.kernel == 'rbf':\n",
        "          self.kernel_function = lambda X, Y: kernels.rbf_kernel(X, Y, self.gamma)\n",
        "        elif self.kernel == 'poly':\n",
        "          self.kernel_function = lambda X, Y: kernels.polynomial_kernel(X, Y, degree=self.gamma)\n",
        "        elif self.kernel == 'linear':\n",
        "            self.kernel_function = lambda X, Y: kernels.linear_kernel(X, Y) + 1\n",
        "        else:\n",
        "            self.kernel_function = kernel\n",
        "\n",
        "\n",
        "        # Choose random reasonable points\n",
        "        self.nmb_pts = len(self.s_train)\n",
        "        if self.reason_points <= 1:\n",
        "          self.reason_pts_index = list(range(int(self.nmb_pts * self.reason_points)))\n",
        "        else:\n",
        "          self.reason_pts_index = list(range(self.reason_points))\n",
        "        self.nmb_reason_pts = len(self.reason_pts_index)\n",
        "\n",
        "    def _construct_problem(self):\n",
        "\n",
        "        # Variable to optimize\n",
        "        self.params = cp.Variable((len(self.reason_pts_index), 1))\n",
        "        # Parameter for Kernel Matrix\n",
        "        self.kernel_matrix = cp.Parameter(shape=(self.x_train.shape[0], len(self.reason_pts_index)))\n",
        "        \n",
        "        self.bias = cp.Variable()\n",
        "\n",
        "        # Loss Function to Minimize (with Regularization)\n",
        "        self.loss = (1 / self.nmb_pts) * cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.params))) + self.l2_beta * cp.square(cp.norm(self.params, 2))\n",
        "        \n",
        "        # Final Problem Formulization\n",
        "        self.prob = cp.Problem(cp.Minimize(self.loss))\n",
        "\n",
        "    def _optimize(self):\n",
        "\n",
        "        self.K_sim = self.kernel_function(self.x_train, self.x_train[self.reason_pts_index])\n",
        "        self.kernel_matrix.value = self.K_sim\n",
        "\n",
        "        if self.solver == 'SCS':\n",
        "            self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=self.verbose, warm_start=True)\n",
        "        elif self.solver == 'ECOS':\n",
        "            try:\n",
        "                self.prob.solve(solver=cp.ECOS, max_iters=self.max_iter, verbose=self.verbose, warm_start=True)\n",
        "            except Exception as e:\n",
        "                self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=self.verbose, warm_start=True)\n",
        "    \n",
        "        self.coef_ = self.params.value.squeeze()\n",
        "    \n",
        "    def predict(self, x_test):\n",
        "        kernel_matrix = self.kernel_function(x_test, self.x_train[self.reason_pts_index])\n",
        "        # Calculate Estimates\n",
        "        y_hat = np.dot(self.coef_, np.transpose(kernel_matrix))\n",
        "        \n",
        "        return np.sign(y_hat)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1OCk8WcqKho"
      },
      "source": [
        "## Section 5.2 Implement Testing Procedure with Fairness Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD53vBvvqKhu"
      },
      "source": [
        "class TestProcedure():\n",
        "    def __init__(self,model):\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def BuildDataset(self,sens_attribute,dataset=dataset,train_size = 1200):\n",
        "        self.sens_attribute = sens_attribute\n",
        "\n",
        "        # Build the Adult Dataset\n",
        "        x_data, y_data, s_data = build_adult_data(dataset,sens_attribute,load_data_size=None)\n",
        "        \n",
        "        # Train Test split size.\n",
        "        train_size = 1200\n",
        "        \n",
        "        # Split data into train and test.\n",
        "        x_train, x_test, y_train, y_test, s_train, s_test = train_test_split(x_data, y_data, s_data, train_size=train_size, shuffle=True)\n",
        "        \n",
        "        self.X_train = x_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "        self.X_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        self.s_train = s_train\n",
        "        self.s_test = s_test\n",
        "                \n",
        "    def BuildModel(self):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        self.model.fit(self.X_train,self.y_train,self.s_train)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        build_time = end_time - start_time\n",
        "        \n",
        "        return build_time\n",
        "        \n",
        "    def RunTest(self,sens_attribute,dataset=dataset):\n",
        "        self.BuildDataset(sens_attribute,dataset=dataset)\n",
        "        build_time = self.BuildModel()\n",
        "        predictions = self.model.predict(self.X_test)\n",
        "        prediction_accuracy = np.equal(self.y_test, predictions).mean()\n",
        "        \n",
        "        ddp,deo = self.compute_fairness_measures(predictions, self.y_test ,self.s_test)\n",
        "        results = {\"BuildTime\":build_time,\"PredictionAccuracy\":prediction_accuracy,\"DDP\":ddp,\"DEO\":deo}\n",
        "        self.PrintResults(results)\n",
        "        return results\n",
        "        \n",
        "    def compute_fairness_measures(self, y_predicted, y_true, sens_attr):\n",
        "        positive_rate_prot = self.get_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
        "        positive_rate_unprot = self.get_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
        "        true_positive_rate_prot = self.get_true_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
        "        true_positive_rate_unprot = self.get_true_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
        "        DDP = positive_rate_unprot - positive_rate_prot\n",
        "        DEO = true_positive_rate_unprot - true_positive_rate_prot\n",
        "\n",
        "        return DDP, DEO\n",
        "\n",
        "    def get_positive_rate(self, y_predicted, y_true):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true.astype(int), y_predicted.astype(int)).ravel()\n",
        "        pr = (tp+fp) / (tp+fp+tn+fn)\n",
        "        return pr\n",
        "\n",
        "    def get_true_positive_rate(self, y_predicted, y_true):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true.astype(int), y_predicted.astype(int)).ravel()\n",
        "        tpr = tp / (tp+fn)\n",
        "        return tpr\n",
        "        \n",
        "    def PrintResults(self,results):\n",
        "      print(\"Sensitive Attribute:\",self.sens_attribute)\n",
        "      print(\"Kernel Type:\",self.model.kernel)\n",
        "      print(\"Loss Func:\",self.model.loss_name)\n",
        "      print(\"Run Time:\",round(results['BuildTime'],4),\"seconds\")\n",
        "      print(\"Prediction Accuracy:\",str(round(results['PredictionAccuracy']*100,4)),\"%\")\n",
        "      print(\"DDP Score:\",str(round(results['DDP'],4)))\n",
        "      print(\"DEO Score:\",str(round(results['DEO'],4)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcxB5j-2qKhv",
        "outputId": "1f3b6795-740d-495c-dcc6-966f9d7fb0f3"
      },
      "source": [
        "baseline_linear_hinge = BaselineModel(kernel='linear',loss_name='hinge')\n",
        "baseline_1_tester = TestProcedure(baseline_linear_hinge)\n",
        "baseline_1_test_results = baseline_1_tester.RunTest(dataset=dataset,sens_attribute='sex')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: sex\n",
            "Kernel Type: linear\n",
            "Loss Func: hinge\n",
            "Run Time: 3.1525 seconds\n",
            "Prediction Accuracy: 75.0242 %\n",
            "DDP Score: 0.4042\n",
            "DEO Score: 0.2572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBUhzonZqKhx",
        "outputId": "548cbae7-a18a-4ba7-b5cd-fb8734679e7a"
      },
      "source": [
        "baseline_rbf_hinge = BaselineModel(kernel='linear',loss_name='hinge')\n",
        "baseline_2_tester = TestProcedure(baseline_rbf_hinge)\n",
        "baseline_2_test_results = baseline_2_tester.RunTest(dataset=dataset,sens_attribute='race')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: race\n",
            "Kernel Type: linear\n",
            "Loss Func: hinge\n",
            "Run Time: 3.3642 seconds\n",
            "Prediction Accuracy: 76.1428 %\n",
            "DDP Score: 0.1487\n",
            "DEO Score: 0.043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7X28nvXX4sN"
      },
      "source": [
        "## Section 5.2.1 Implement Baseline Hyperparameter Grid Search **for Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ktdFfs1ecY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "4cbcd02d-2605-4ad2-d8ab-9ab4ebfd5cfb"
      },
      "source": [
        "# regularization parameter beta\n",
        "sens_attribute = 'sex'\n",
        "\n",
        "grid_search_model = BaselineModel()\n",
        "\n",
        "beta_params = [0.0001, 0.001, 0.01] # For Linear Kernel\n",
        "gamma_params = [0.01, 0.1, 1] # For RBF Kernel\n",
        "kernel_params = ['linear','rbf']\n",
        "cv_params = {'l2_beta': beta_params,'gamma': gamma_params,'kernel':kernel_params}\n",
        "\n",
        "x_data, y_data, s_data = build_adult_data(sens_attribute,load_data_size=None)\n",
        "x_train, x_test, y_train, y_test, s_train, s_test = train_test_split(x_data, y_data, s_data, train_size=1200, shuffle=True)\n",
        "\n",
        "grid_clf = GridSearchCV(grid_search_model,cv_params, cv=4, n_jobs=1, scoring='accuracy')\n",
        "grid_clf.fit(x_train, y_train, s_train = s_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8ed9dc73362d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcv_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'l2_beta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgamma_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkernel_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_adult_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msens_attribute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_data_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-35da641aebb8>\u001b[0m in \u001b[0;36mbuild_adult_data\u001b[0;34m(dataset, sens_attribute, load_data_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;31m# Convert Binary Mapping of Sensitive Attribute to {1,-1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'apply'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tb_Vl-QO_G8"
      },
      "source": [
        "grid_clf.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi8pObrToe5k"
      },
      "source": [
        "## Section 5.2.2 Implement Baseline Hyperparameter Grid Search **for DDP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3wbd6-RuB8t"
      },
      "source": [
        "from sklearn.metrics.scorer import make_scorer"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR-Dw7NJr9W_"
      },
      "source": [
        "grid_split_counter = 1\n",
        "def get_positive_rate(y_predicted, y_true):\n",
        "  \n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
        "  pr = (tp+fp) / (tp+fp+tn+fn)\n",
        "  return pr\n",
        "\n",
        "def DDP_Grid_Scoring(y_true,y_predicted,sens_attr,size):\n",
        "  global grid_split_counter\n",
        "  chunk_size = 1200/size\n",
        "  sens_attribute = sens_attr[int((grid_split_counter - 1)*chunk_size): int(grid_split_counter*chunk_size)]\n",
        "  if grid_split_counter == size:\n",
        "    grid_split_counter = 1\n",
        "  else:\n",
        "    grid_split_counter += 1\n",
        "  positive_rate_prot = get_positive_rate(y_predicted[sens_attribute==-1], y_true[sens_attribute==-1])\n",
        "  positive_rate_unprot = get_positive_rate(y_predicted[sens_attribute==1], y_true[sens_attribute==1])\n",
        "  DDP = abs(positive_rate_unprot - positive_rate_prot)\n",
        "  return DDP\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjlXeTgjRNkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00d48d9-f480-4479-d53e-d4ff0bd99150"
      },
      "source": [
        "# regularization parameter beta\n",
        "sens_attribute = 'sex'\n",
        "size = 4\n",
        "\n",
        "grid_search_model = BaselineModel()\n",
        "\n",
        "beta_params = [0.0001, 0.001, 0.01] # For Linear Kernel\n",
        "gamma_params = [0.01, 0.1, 1] # For RBF Kernel\n",
        "kernel_params = ['linear','rbf']\n",
        "cv_params = {'l2_beta': beta_params,'gamma': gamma_params,'kernel':kernel_params}\n",
        "\n",
        "x_data, y_data, s_data = build_adult_data(dataset,sens_attribute,load_data_size=None)\n",
        "x_train, x_test, y_train, y_test, s_train, s_test = train_test_split(x_data, y_data, s_data, train_size=1200, shuffle=True)\n",
        "\n",
        "\n",
        "DDP_scorer = make_scorer(DDP_Grid_Scoring, greater_is_better=False, sens_attr = s_train, size = size)\n",
        "\n",
        "fairness_grid_clf = GridSearchCV(grid_search_model,cv_params, cv=size, n_jobs=1, scoring=DDP_scorer)\n",
        "fairness_grid_clf.fit(x_train, y_train, s_train = s_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=4, error_score=nan,\n",
              "             estimator=BaselineModel(gamma=0.1, kernel='linear', l2_beta=0.001,\n",
              "                                     lambda_max=1, loss_name='hinge',\n",
              "                                     max_iter=3000, reason_points=0.5,\n",
              "                                     solver='SCS', verbose=False),\n",
              "             iid='deprecated', n_jobs=1,\n",
              "             param_grid={'gamma': [0.01, 0.1, 1], 'kernel': ['linear', 'rbf'],\n",
              "                         'l2_beta': [0.0001, 0.001, 0.01]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=make_scorer(DDP_Grid_Scoring, greater_is_better=False, sens_attr=[ 1  1 -1 ...  1 -1 -1], size=4),\n",
              "             verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSweey0Jtw88",
        "outputId": "8ab831c5-f87f-4a23-b309-071c3201adf7"
      },
      "source": [
        "fairness_grid_clf.cv_results_"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([1.20601726, 1.11244941, 1.06649196, 4.98668581, 1.86064446,\n",
              "        1.68333358, 1.15317744, 1.02378315, 1.01494241, 2.19605738,\n",
              "        1.52874708, 1.70175511, 1.2158429 , 1.05668122, 1.03966737,\n",
              "        2.82413894, 1.22350794, 1.35706484]),\n",
              " 'mean_score_time': array([0.006706  , 0.00896037, 0.00438321, 0.01348609, 0.00901544,\n",
              "        0.01569533, 0.00434381, 0.00434524, 0.00461125, 0.00924152,\n",
              "        0.01148462, 0.01131362, 0.00433844, 0.00438136, 0.00437474,\n",
              "        0.01370716, 0.01573914, 0.00916189]),\n",
              " 'mean_test_score': array([-0.44624491, -0.44624491, -0.44624491, -0.44624491, -0.44624491,\n",
              "        -0.43609111, -0.44624491, -0.44624491, -0.44624491, -0.40696772,\n",
              "        -0.43314974, -0.44624491, -0.44624491, -0.44624491, -0.44624491,\n",
              "        -0.37175456, -0.3770455 , -0.40546275]),\n",
              " 'param_gamma': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1,\n",
              "                    0.1, 0.1, 1, 1, 1, 1, 1, 1],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_kernel': masked_array(data=['linear', 'linear', 'linear', 'rbf', 'rbf', 'rbf',\n",
              "                    'linear', 'linear', 'linear', 'rbf', 'rbf', 'rbf',\n",
              "                    'linear', 'linear', 'linear', 'rbf', 'rbf', 'rbf'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_l2_beta': masked_array(data=[0.0001, 0.001, 0.01, 0.0001, 0.001, 0.01, 0.0001,\n",
              "                    0.001, 0.01, 0.0001, 0.001, 0.01, 0.0001, 0.001, 0.01,\n",
              "                    0.0001, 0.001, 0.01],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'gamma': 0.01, 'kernel': 'linear', 'l2_beta': 0.0001},\n",
              "  {'gamma': 0.01, 'kernel': 'linear', 'l2_beta': 0.001},\n",
              "  {'gamma': 0.01, 'kernel': 'linear', 'l2_beta': 0.01},\n",
              "  {'gamma': 0.01, 'kernel': 'rbf', 'l2_beta': 0.0001},\n",
              "  {'gamma': 0.01, 'kernel': 'rbf', 'l2_beta': 0.001},\n",
              "  {'gamma': 0.01, 'kernel': 'rbf', 'l2_beta': 0.01},\n",
              "  {'gamma': 0.1, 'kernel': 'linear', 'l2_beta': 0.0001},\n",
              "  {'gamma': 0.1, 'kernel': 'linear', 'l2_beta': 0.001},\n",
              "  {'gamma': 0.1, 'kernel': 'linear', 'l2_beta': 0.01},\n",
              "  {'gamma': 0.1, 'kernel': 'rbf', 'l2_beta': 0.0001},\n",
              "  {'gamma': 0.1, 'kernel': 'rbf', 'l2_beta': 0.001},\n",
              "  {'gamma': 0.1, 'kernel': 'rbf', 'l2_beta': 0.01},\n",
              "  {'gamma': 1, 'kernel': 'linear', 'l2_beta': 0.0001},\n",
              "  {'gamma': 1, 'kernel': 'linear', 'l2_beta': 0.001},\n",
              "  {'gamma': 1, 'kernel': 'linear', 'l2_beta': 0.01},\n",
              "  {'gamma': 1, 'kernel': 'rbf', 'l2_beta': 0.0001},\n",
              "  {'gamma': 1, 'kernel': 'rbf', 'l2_beta': 0.001},\n",
              "  {'gamma': 1, 'kernel': 'rbf', 'l2_beta': 0.01}],\n",
              " 'rank_test_score': array([7, 7, 7, 7, 7, 6, 7, 7, 7, 4, 5, 7, 7, 7, 7, 1, 2, 3], dtype=int32),\n",
              " 'split0_test_score': array([-0.38750339, -0.38750339, -0.38750339, -0.38750339, -0.38750339,\n",
              "        -0.39590675, -0.38750339, -0.38750339, -0.38750339, -0.38750339,\n",
              "        -0.38750339, -0.38750339, -0.38750339, -0.38750339, -0.38750339,\n",
              "        -0.39102738, -0.40363242, -0.38330171]),\n",
              " 'split1_test_score': array([-0.48910748, -0.48910748, -0.48910748, -0.48910748, -0.48910748,\n",
              "        -0.47052079, -0.48910748, -0.48910748, -0.48910748, -0.4328043 ,\n",
              "        -0.43672681, -0.48910748, -0.48910748, -0.48910748, -0.48910748,\n",
              "        -0.37849255, -0.39611369, -0.44161487]),\n",
              " 'split2_test_score': array([-0.41832625, -0.41832625, -0.41832625, -0.41832625, -0.41832625,\n",
              "        -0.41989259, -0.41832625, -0.41832625, -0.41832625, -0.39841128,\n",
              "        -0.41832625, -0.41832625, -0.41832625, -0.41832625, -0.41832625,\n",
              "        -0.37704184, -0.39997762, -0.39841128]),\n",
              " 'split3_test_score': array([-0.49004252, -0.49004252, -0.49004252, -0.49004252, -0.49004252,\n",
              "        -0.45804431, -0.49004252, -0.49004252, -0.49004252, -0.40915194,\n",
              "        -0.49004252, -0.49004252, -0.49004252, -0.49004252, -0.49004252,\n",
              "        -0.34045648, -0.30845827, -0.39852316]),\n",
              " 'std_fit_time': array([0.08265414, 0.06410312, 0.0207726 , 1.02216056, 0.33808655,\n",
              "        0.2479211 , 0.07424362, 0.016233  , 0.00327286, 0.32680313,\n",
              "        0.25840674, 0.13299839, 0.12575353, 0.07771461, 0.03375969,\n",
              "        0.73264765, 0.12847848, 0.1489612 ]),\n",
              " 'std_score_time': array([2.35399648e-03, 3.11695329e-03, 3.07562277e-05, 4.45238811e-03,\n",
              "        8.26631116e-05, 3.80075482e-03, 6.17136660e-05, 3.48855247e-05,\n",
              "        1.10502703e-04, 2.14486658e-04, 3.76872988e-03, 3.94353074e-03,\n",
              "        9.86504770e-05, 3.68248185e-05, 1.68351206e-05, 4.70734975e-03,\n",
              "        3.92201565e-03, 1.88822425e-04]),\n",
              " 'std_test_score': array([0.04468067, 0.04468067, 0.04468067, 0.04468067, 0.04468067,\n",
              "        0.02976794, 0.04468067, 0.04468067, 0.04468067, 0.01676583,\n",
              "        0.03725892, 0.04468067, 0.04468067, 0.04468067, 0.04468067,\n",
              "        0.0188704 , 0.03968801, 0.02177136])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UxXTvGhUezC"
      },
      "source": [
        "## Section 5.3 Implement Lohaus' SearchFair Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIvoGBIdIBv3"
      },
      "source": [
        "class SearchFair(BaseEstimator):\n",
        "    \"\"\"SearchFair\n",
        "    Parameters\n",
        "    ----------\n",
        "    fairness_notions: string\n",
        "        The name of the fairness notion that the classifier should respect. 'DDP' or 'DEO' can be used.\n",
        "    fairness_regularizer: string\n",
        "        The name of the fairness relaxation that is used as a regularizer. It can be 'linear', or 'wu'. For 'wu', the 'wu_bound' can be chosen.\n",
        "    wu_bound: string\n",
        "        The name of the function that is used in the bounds of Wu et al. It can be 'hinge', 'logistic', 'squared', 'exponential'\n",
        "    reg_beta: float\n",
        "        Regularization parameter Beta for the l2 regularization.\n",
        "    kernel: string\n",
        "        The kind of kernel that is used. It can be 'linear', 'rbf' or 'poly'. For 'rbf' and 'poly', the parameter gamma can be used.\n",
        "    gamma: float\n",
        "        For kernel='rbf', gamma is the kernel width, for kernel='poly', gamma is the degree.\n",
        "    loss_name: string\n",
        "        The name of the loss used. Possible values: 'hinge', 'logistic', 'squared', 'exponential'\n",
        "    lambda_max: float\n",
        "        The value of lambda_max for the start of the binary search.\n",
        "    max_iter: int\n",
        "        The number of iterations of the solver chosen.\n",
        "    reason_points: float\n",
        "        The ratio of points used as reasonable points for the similarity-based approach of SearchFair.\n",
        "    stop_criterion: float\n",
        "        If SearchFair finds a classifier that is at least as fair as 'stop_criterion', than it stops the search.\n",
        "    max_search_iter: int\n",
        "        The number of iterations for the binary search.\n",
        "    solver: string\n",
        "        The solver that is used by cvxpy. It can be 'SCS' or 'ECOS'.\n",
        "    verbose: boolean\n",
        "    Attributes\n",
        "    ----------\n",
        "    coef_: numpy array\n",
        "        An array containing the trained weights for each reasonable point.\n",
        "    reason_pts_index: numpy array\n",
        "        An array containing the indices of the reasonable points in the training data.\n",
        "    Notes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fairness_notion='DDP', fairness_regularizer='wu', wu_bound='hinge', reg_beta=0.001, kernel='linear', gamma=None, loss_name='hinge', lambda_max=1, max_iter=3000, reason_points=0.5, stop_criterion=0.01, max_search_iter=10, solver='SCS', verbose=False):\n",
        "\n",
        "        self.reg_beta = reg_beta\n",
        "        self.fairness_notion = fairness_notion\n",
        "        self.max_iter = max_iter\n",
        "        self.max_search_iter = max_search_iter\n",
        "        self.solver = solver\n",
        "        self.verbose = verbose\n",
        "        self.stop_criterion = stop_criterion\n",
        "        self.reason_points = reason_points\n",
        "        self.lambda_max = lambda_max\n",
        "        self.wu_bound = wu_bound\n",
        "        self.fairness_regularizer = fairness_regularizer\n",
        "        self.wu_bound = wu_bound\n",
        "        self.gamma = gamma\n",
        "        self.loss_name = loss_name\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def fit(self, x_train, y_train, s_train=None):\n",
        "        \"\"\"Fits SearchFair on the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train: numpy array\n",
        "            The features of the training data with shape=(number_points,number_features).\n",
        "        y_train: numpy array\n",
        "            The class labels of the training data with shape=(number_points,).\n",
        "        s_train: numpy array\n",
        "            The binary sensitive attributes of the training data with shape=(number_points,).\n",
        "        Returns\n",
        "        ----------\n",
        "        self: object\n",
        "        \"\"\"\n",
        "\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.s_train = s_train\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Preprocessing...\")\n",
        "        self._preprocess()\n",
        "\n",
        "        lbda_min, lbda_max = 0, self.lambda_max\n",
        "\n",
        "        def learn(reg, bound='upper'):\n",
        "            # If bound is None, we have decided which one to use, and we are in the middle of the binary search\n",
        "\n",
        "            self.fairness_lambda = reg\n",
        "            if bound is not None:\n",
        "                self._construct_problem(bound=bound)\n",
        "            self._optimize()\n",
        "            DDP, DEO = self.compute_fairness_measures(self.predict(x_train), y_train, s_train)\n",
        "            if self.fairness_notion == 'DDP':\n",
        "                fair_value = DDP\n",
        "            else:\n",
        "                fair_value = DEO\n",
        "            if self.verbose: print(\"Obtained:\",self.fairness_notion, \"= %0.4f with lambda = %0.4f\" % (fair_value, reg))\n",
        "            return fair_value, self.coef_.copy()\n",
        "\n",
        "        criterion = False\n",
        "\n",
        "        bound = 'upper' # even though an upper bound is specified, since lambda_min is 0, it falls away\n",
        "        if self.verbose: print(\"Testing lambda_min: %0.2f\" % lbda_min)\n",
        "        min_fair_measure, min_alpha = learn(lbda_min, bound=bound)\n",
        "        if np.sign(min_fair_measure) < 0: bound = 'lower'\n",
        "        if self.verbose: print(\"Testing lambda_max: %0.2f\" % lbda_max)\n",
        "        max_fair_measure, max_alpha = learn(lbda_max, bound)\n",
        "\n",
        "        if np.abs(min_fair_measure) < np.abs(max_fair_measure):\n",
        "            best_lbda, best_fair_measure = lbda_min, min_fair_measure\n",
        "            best_alpha = min_alpha\n",
        "        else:\n",
        "            best_lbda, best_fair_measure = lbda_max, max_fair_measure\n",
        "            best_alpha = max_alpha\n",
        "        if  np.abs(best_fair_measure) < self.stop_criterion:\n",
        "            print(\"Classifier is fair enough with lambda = {:.4f}\".format(best_lbda))\n",
        "        elif np.sign(min_fair_measure) == np.sign(max_fair_measure):\n",
        "            print('Fairness value has the same sign for lambda_min and lambda_max.')\n",
        "            print('Either try a different fairness regularizer or change the values of lambda_min and lambda_max') # Possibly, there could be a few more tries by reducing lambda.\n",
        "        else:\n",
        "            search_iter = 0\n",
        "            if self.verbose: print(\"Starting Binary Search...\")\n",
        "            while not criterion and search_iter < self.max_search_iter:\n",
        "                lbda_new = (lbda_min + lbda_max) / 2\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(10*'-'+\"Iteration #%0.0f\" % search_iter + 10*'-')\n",
        "                    print(\"Testing new Lambda: %0.4f\" % lbda_new)\n",
        "\n",
        "                new_rd, new_alpha = learn(lbda_new, None)\n",
        "                if np.abs(new_rd) < np.abs(best_fair_measure):\n",
        "                    best_fair_measure = new_rd\n",
        "                    best_lbda = lbda_new\n",
        "                    best_alpha = new_alpha.copy()\n",
        "\n",
        "                if np.sign(new_rd) == np.sign(min_fair_measure):\n",
        "                    min_fair_measure = new_rd\n",
        "                    lbda_min = lbda_new\n",
        "                else:\n",
        "                    max_fair_measure = new_rd\n",
        "                    lbda_max = lbda_new\n",
        "                if np.abs(new_rd) < self.stop_criterion:\n",
        "                    criterion = True\n",
        "\n",
        "                search_iter += 1\n",
        "            if search_iter==self.max_search_iter and self.verbose:\n",
        "                print(\"Hit maximum iterations of Binary Search.\")\n",
        "            elif self.verbose:\n",
        "                print(\"Sufficient fairness obtained before maximum iterations were reached.\")\n",
        "\n",
        "        if self.verbose: print(10*'-'+\"Found Lambda %0.4f with fairness %0.4f\" % (best_lbda, best_fair_measure)+10*'-')\n",
        "        self.coef_ = best_alpha.copy()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\"Predict the label of test data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_test: numpy array\n",
        "            The features of the test data with shape=(number_points,number_features).\n",
        "        Returns\n",
        "        ----------\n",
        "        y_hat: numpy array\n",
        "            The predicted class labels with shape=(number_points,).\n",
        "        \"\"\"\n",
        "        kernel_matr = self.kernel_function(x_test, self.x_train[self.reason_pts_index])\n",
        "        y_hat = np.dot(self.coef_, np.transpose(kernel_matr))\n",
        "        return np.sign(y_hat)\n",
        "\n",
        "    def _preprocess(self):\n",
        "        \"\"\"Setting the attributes loss_func, kernel_function, and weight_vector,\n",
        "        which depends on the fairness notion, and is used in fairness related objects.\n",
        "        \"\"\"\n",
        "        self.coef_ = None\n",
        "        self.fairness_lambda = 0\n",
        "        if self.loss_name == 'logistic':\n",
        "            self.loss_func = lambda z: cp.logistic(-z)\n",
        "        elif self.loss_name == 'hinge':\n",
        "            self.loss_func = lambda z: cp.pos(1.0 - z)\n",
        "        elif self.loss_name == 'squared':\n",
        "            self.loss_func = lambda z: cp.square(-z)\n",
        "        elif self.loss_name == 'exponential':\n",
        "            self.loss_func = lambda z: cp.exp(-z)\n",
        "        else:\n",
        "            print('Using default loss: hinge loss.')\n",
        "            self.loss_func = lambda z: cp.pos(1.0 - z)\n",
        "\n",
        "        if self.kernel == 'rbf':\n",
        "            self.kernel_function = lambda X, Y: kernels.rbf_kernel(X, Y, self.gamma)\n",
        "        elif self.kernel == 'poly':\n",
        "            self.kernel_function = lambda X, Y: kernels.polynomial_kernel(X, Y, degree=self.gamma)\n",
        "        elif self.kernel == 'linear':\n",
        "            self.kernel_function = lambda X, Y: kernels.linear_kernel(X, Y) + 1\n",
        "        else:\n",
        "            self.kernel_function = kernel\n",
        "\n",
        "        if self.wu_bound == 'logistic':\n",
        "            self.cvx_kappa = lambda z: cp.logistic(z)\n",
        "            self.cvx_delta = lambda z: 1 - cp.logistic(-z)\n",
        "        elif self.wu_bound == 'hinge':\n",
        "            self.cvx_kappa = lambda z: cp.pos(1 + z)\n",
        "            self.cvx_delta = lambda z: 1 - cp.pos(1 - z)\n",
        "        elif self.wu_bound == 'squared':\n",
        "            self.cvx_kappa = lambda z: cp.square(1 + z)\n",
        "            self.cvx_delta = lambda z: 1 - cp.square(1 - z)\n",
        "        elif self.wu_bound == 'exponential':\n",
        "            self.cvx_kappa = lambda z: cp.exp(z)\n",
        "            self.cvx_delta = lambda z: 1 - cp.exp(-z)\n",
        "        else:\n",
        "            print('Using default bound with hinge.')\n",
        "            self.cvx_kappa = lambda z: cp.pos(1 + z)\n",
        "            self.cvx_delta = lambda z: 1 - cp.pos(1 - z)\n",
        "\n",
        "        self.nmb_pts = len(self.s_train)\n",
        "        self.nmb_unprotected = np.sum(self.s_train == 1)\n",
        "        self.prob_unprot = self.nmb_unprotected / self.nmb_pts\n",
        "        self.prob_prot = 1 - self.prob_unprot\n",
        "\n",
        "        self.nmb_pos = np.sum(self.y_train == 1)\n",
        "        self.nmb_prot_pos = np.sum(self.y_train[self.s_train == -1] == 1)\n",
        "        self.prob_prot_pos = self.nmb_prot_pos / self.nmb_pos\n",
        "        self.prob_unprot_pos = 1 - self.prob_prot_pos\n",
        "\n",
        "        # Create weights that are necessary for the fairness constraint\n",
        "        if self.fairness_notion == 'DDP':\n",
        "            normalizer = self.nmb_pts\n",
        "            self.weight_vector = np.array(\n",
        "                [1.0 / self.prob_prot if self.s_train[i] == -1 else 1.0 / self.prob_unprot for i in range(len(self.s_train))]).reshape(-1,1)\n",
        "            self.weight_vector = (1 / normalizer) * self.weight_vector\n",
        "        elif self.fairness_notion == 'DEO':\n",
        "            normalizer = self.nmb_pos\n",
        "            self.weight_vector = np.array(\n",
        "                [1.0 / self.prob_prot_pos if self.s_train[i] == -1 else 1.0 / self.prob_unprot_pos for i in range(len(self.s_train))]).reshape(-1, 1)\n",
        "            self.weight_vector = 0.5 * (self.y_train.reshape(-1, 1) + 1) * self.weight_vector\n",
        "            self.weight_vector = (1 / normalizer) * self.weight_vector\n",
        "\n",
        "        # Choose random reasonable points\n",
        "        if self.reason_points <= 1:\n",
        "            self.reason_pts_index = list(range(int(self.nmb_pts * self.reason_points)))\n",
        "        else:\n",
        "            self.reason_pts_index = list(range(self.reason_points))\n",
        "        self.nmb_reason_pts = len(self.reason_pts_index)\n",
        "\n",
        "    def _construct_problem(self, bound='upper'):\n",
        "        \"\"\" Construct the cvxpy minimization problem.\n",
        "        It depends on the fairness regularizer chosen.\n",
        "        \"\"\"\n",
        "\n",
        "        # Variable to optimize\n",
        "        self.alpha_var = cp.Variable((len(self.reason_pts_index), 1))\n",
        "        # Parameter for Kernel Matrix\n",
        "        self.kernel_matrix = cp.Parameter(shape=(self.x_train.shape[0], len(self.reason_pts_index)))\n",
        "        self.fair_reg_cparam = cp.Parameter(nonneg=True)\n",
        "\n",
        "\n",
        "        # Form SVM with L2 regularization\n",
        "        if self.fairness_lambda == 0:\n",
        "            self.loss = cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + self.reg_beta * self.nmb_pts * cp.square(\n",
        "                cp.norm(self.alpha_var, 2))\n",
        "        else:\n",
        "            sy_hat = cp.multiply(self.s_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var)\n",
        "\n",
        "            if self.fairness_regularizer == 'wu':\n",
        "                if bound == 'upper':\n",
        "                    fairness_relaxation = cp.sum(cp.multiply(self.weight_vector, self.cvx_kappa(sy_hat))) - 1\n",
        "                else:\n",
        "                    fairness_relaxation = -1 * cp.sum(cp.multiply(self.weight_vector, self.cvx_delta(sy_hat))) - 1\n",
        "\n",
        "\n",
        "            elif self.fairness_regularizer == 'linear':\n",
        "                if bound == 'upper':\n",
        "                    fairness_relaxation = cp.sum(cp.multiply(self.weight_vector, self.kernel_matrix @ self.alpha_var))\n",
        "                else:\n",
        "                    fairness_relaxation = -1 * cp.sum(cp.multiply(self.weight_vector, self.kernel_matrix @ self.alpha_var))\n",
        "\n",
        "            if self.reg_beta == 0:\n",
        "                self.loss = (1/self.nmb_pts) * cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + \\\n",
        "                                self.fair_reg_cparam * fairness_relaxation\n",
        "            else:\n",
        "                self.loss = (1 / self.nmb_pts) * cp.sum(self.loss_func(cp.multiply(self.y_train.reshape(-1, 1), self.kernel_matrix @ self.alpha_var))) + \\\n",
        "                            self.fair_reg_cparam * fairness_relaxation + self.reg_beta * cp.square(cp.norm(self.alpha_var, 2))\n",
        "\n",
        "        self.prob = cp.Problem(cp.Minimize(self.loss))\n",
        "\n",
        "    def _optimize(self):\n",
        "        \"\"\"Conduct the optimization of the created problem by using ECOS or SCS\n",
        "        with cvxpy. \n",
        "        \"\"\"\n",
        "\n",
        "        # Compute and initialize kernel matrix\n",
        "        self.K_sim = self.kernel_function(self.x_train, self.x_train[self.reason_pts_index])\n",
        "        self.kernel_matrix.value = self.K_sim\n",
        "        self.fair_reg_cparam.value = self.fairness_lambda\n",
        "\n",
        "        if self.verbose == 2:\n",
        "            verbose = True\n",
        "        else:\n",
        "            verbose = False\n",
        "        if self.solver == 'SCS':\n",
        "            self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
        "        elif self.solver == 'ECOS':\n",
        "            try:\n",
        "                self.prob.solve(solver=cp.ECOS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
        "            except Exception as e:\n",
        "                self.prob.solve(solver=cp.SCS, max_iters=self.max_iter, verbose=verbose, warm_start=True)\n",
        "        if verbose:\n",
        "            print('status %s ' % self.prob.status)\n",
        "            print('value %s ' % self.prob.value)\n",
        "        self.coef_ = self.alpha_var.value.squeeze()\n",
        "    def compute_fairness_measures(self, y_predicted, y_true, sens_attr):\n",
        "        \"\"\"Compute value of demographic parity and equality of opportunity for given predictions.\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_predicted: numpy array\n",
        "            The predicted class labels of shape=(number_points,).\n",
        "        y_true: numpy array\n",
        "            The true class labels of shape=(number_points,).\n",
        "        sens_attr: numpy array\n",
        "            The sensitive labels of shape=(number_points,).\n",
        "        Returns\n",
        "        ----------\n",
        "        DDP: float\n",
        "            The difference of demographic parity.\n",
        "        DEO: float\n",
        "            The difference of equality of opportunity.\n",
        "        \"\"\"\n",
        "        positive_rate_prot = self.get_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
        "        positive_rate_unprot = self.get_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
        "        true_positive_rate_prot = self.get_true_positive_rate(y_predicted[sens_attr==-1], y_true[sens_attr==-1])\n",
        "        true_positive_rate_unprot = self.get_true_positive_rate(y_predicted[sens_attr==1], y_true[sens_attr==1])\n",
        "        DDP = positive_rate_unprot - positive_rate_prot\n",
        "        DEO = true_positive_rate_unprot - true_positive_rate_prot\n",
        "\n",
        "        return DDP, DEO\n",
        "\n",
        "    def get_positive_rate(self, y_predicted, y_true):\n",
        "        \"\"\"Compute the positive rate for given predictions of the class label.\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_predicted: numpy array\n",
        "            The predicted class labels of shape=(number_points,).\n",
        "        y_true: numpy array\n",
        "            The true class labels of shape=(number_points,).\n",
        "        Returns\n",
        "        ---------\n",
        "        pr: float\n",
        "            The positive rate.\n",
        "        \"\"\"\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
        "        pr = (tp+fp) / (tp+fp+tn+fn)\n",
        "        return pr\n",
        "\n",
        "    def get_true_positive_rate(self, y_predicted, y_true):\n",
        "        \"\"\"Compute the true positive rate for given predictions of the class label.\n",
        "        Parameters\n",
        "        ----------\n",
        "        y_predicted: numpy array\n",
        "            The predicted class labels of shape=(number_points,).\n",
        "        y_true: numpy array\n",
        "            The true class labels of shape=(number_points,).\n",
        "        Returns\n",
        "        ---------\n",
        "        tpr: float\n",
        "            The true positive rate.\n",
        "        \"\"\"\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
        "        tpr = tp / (tp+fn)\n",
        "        return tpr"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4AyLJ9cbL57"
      },
      "source": [
        "# Section 6\tResults & Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-HDdM5nWMDz",
        "outputId": "f5d62cfa-7e69-42f3-d232-1c6fa541a54e"
      },
      "source": [
        "fairness_notion = 'DDP'  \n",
        "kernel = 'linear' \n",
        "verbose = False\n",
        "\n",
        "SearchFair_Sex_DDP_LinearKernel_Hinge = SearchFair(kernel=kernel, fairness_notion=fairness_notion, verbose=verbose)\n",
        "SearchFair_tester = TestProcedure(SearchFair_Sex_DDP_LinearKernel_Hinge)\n",
        "SearchFair_tester_results = SearchFair_tester.RunTest(dataset=dataset,sens_attribute='sex')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: sex\n",
            "Kernel Type: linear\n",
            "Loss Func: hinge\n",
            "Run Time: 79.2783 seconds\n",
            "Prediction Accuracy: 63.4236 %\n",
            "DDP Score: 0.0263\n",
            "DEO Score: -0.038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hVkv3KGYYhQ",
        "outputId": "21cd013b-8809-475a-ecb5-da2ea1704176"
      },
      "source": [
        "fairness_notion = 'DDP'  \n",
        "kernel = 'linear' \n",
        "verbose = False\n",
        "\n",
        "SearchFair_Race_DDP_LinearKernel_Hinge = SearchFair(kernel=kernel, fairness_notion=fairness_notion, verbose=verbose)\n",
        "SearchFair_tester = TestProcedure(SearchFair_Race_DDP_LinearKernel_Hinge)\n",
        "SearchFair_tester_results = SearchFair_tester.RunTest(dataset=dataset,sens_attribute='race')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: race\n",
            "Kernel Type: linear\n",
            "Loss Func: hinge\n",
            "Run Time: 72.4254 seconds\n",
            "Prediction Accuracy: 58.7902 %\n",
            "DDP Score: -0.019\n",
            "DEO Score: 0.0229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkOTxQPbKGO",
        "outputId": "35b461fb-2346-4ffd-9634-17206b02fe58"
      },
      "source": [
        "fairness_notion = 'DDP'  \n",
        "kernel = 'rbf' \n",
        "verbose = False\n",
        "\n",
        "SearchFair_Sex_DDP_RBFKernel_Hinge = SearchFair(kernel=kernel, fairness_notion=fairness_notion, verbose=verbose)\n",
        "SearchFair_tester = TestProcedure(SearchFair_Sex_DDP_RBFKernel_Hinge)\n",
        "SearchFair_tester_results = SearchFair_tester.RunTest(dataset=dataset,sens_attribute='sex')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: sex\n",
            "Kernel Type: rbf\n",
            "Loss Func: hinge\n",
            "Run Time: 16.1215 seconds\n",
            "Prediction Accuracy: 63.3062 %\n",
            "DDP Score: 0.0286\n",
            "DEO Score: -0.0416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24Uf2WtQb6Mr",
        "outputId": "3946a1f1-3bb7-44e3-a18a-855b9d502718"
      },
      "source": [
        "fairness_notion = 'DDP'  \n",
        "kernel = 'rbf' \n",
        "verbose = False\n",
        "\n",
        "SearchFair_Race_DDP_RBFKernel_Hinge = SearchFair(kernel=kernel, fairness_notion=fairness_notion, verbose=verbose)\n",
        "SearchFair_tester = TestProcedure(SearchFair_Race_DDP_RBFKernel_Hinge)\n",
        "SearchFair_tester_results = SearchFair_tester.RunTest(dataset=dataset,sens_attribute='race')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sensitive Attribute: race\n",
            "Kernel Type: rbf\n",
            "Loss Func: hinge\n",
            "Run Time: 50.1494 seconds\n",
            "Prediction Accuracy: 72.8974 %\n",
            "DDP Score: 0.0542\n",
            "DEO Score: -0.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbJ9K7HjcMjv"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}